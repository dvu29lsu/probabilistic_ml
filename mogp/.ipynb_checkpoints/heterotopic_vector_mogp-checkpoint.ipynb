{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70af8777-89f9-4275-9d3e-d3e4215976dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement the MOGP with the heterotopic case\n",
    "\n",
    "## Modeling in term of two functions which is quadratic function, convex\n",
    "## D1 and D2 can be different all points different, some points can be overlap (in training)\n",
    "## Testing data will totally differents\n",
    "\n",
    "## 1. Load the Hessian from .mat file\n",
    "\n",
    "## 2. Generate function\n",
    "\n",
    "## 3. Generate data function for each functions and sharing training points\n",
    "\n",
    "## 4. Construct the covariance structure between two functions (may just use the library for this)\n",
    "\n",
    "## 5. Prepare data for training using two datasets.\n",
    "\n",
    "## 6. Training the model + get results in test (RMSE + NLL) for predictions of each function\n",
    "\n",
    "## 7. Train separate GP & compare the result with training separately model\n",
    "\n",
    "## 8. Vary data -> see the different vary in the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690d03a4-f55f-4100-aa50-3b31b056f253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpytorch\n",
      "  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting jaxtyping (from gpytorch)\n",
      "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from gpytorch) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\longv\\anaconda3\\lib\\site-packages (from gpytorch) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from gpytorch) (1.12.0)\n",
      "Collecting linear-operator>=0.6 (from gpytorch)\n",
      "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from linear-operator>=0.6->gpytorch) (2.7.0+cu118)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from scipy>=1.6.0->gpytorch) (1.26.4)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
      "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from scikit-learn->gpytorch) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from scikit-learn->gpytorch) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\longv\\anaconda3\\lib\\site-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\longv\\anaconda3\\lib\\site-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\longv\\anaconda3\\lib\\site-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\longv\\anaconda3\\lib\\site-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (75.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\longv\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0->linear-operator>=0.6->gpytorch) (2.1.3)\n",
      "Downloading gpytorch-1.14-py3-none-any.whl (277 kB)\n",
      "Downloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
      "Downloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
      "Downloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: wadler-lindig, jaxtyping, linear-operator, gpytorch\n",
      "Successfully installed gpytorch-1.14 jaxtyping-0.3.2 linear-operator-0.6 wadler-lindig-0.1.7\n"
     ]
    }
   ],
   "source": [
    "# !pip install gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "c6460029-9f28-4362-a40c-4d6af82edcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "484f1dda-e514-4d73-b06c-5e16bd10fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "002afd4f-8320-41e5-9697-3ae75f819ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file_path = r\"C:\\\\Users\\\\longv\\\\research\\\\mogp\\\\data_quad\\\\data\\\\10 Agents\\\\p5\\\\Set1.mat\"\n",
    "mat_data = loadmat(mat_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "4d8fe2a5-60eb-4c61-883e-d8a3de3816ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the hession matrix \n",
    "hessian_matrix = mat_data[\"Phi\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "6e7b0728-e28f-453b-aa98-242b23763ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0); np.random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "9f049529-0ea1-4655-8ca6-cfc61cced7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose function\n",
    "idx0, idx1 = 0, 3\n",
    "\n",
    "# choose input range\n",
    "x_range = (0, 0.5) ## the input range could be larger but due to the limitation of data -> try to keep it in small range\n",
    "\n",
    "#training and testsizes\n",
    "\n",
    "n_train_task = 60     # try small point first\n",
    "n_test_task  = 120          # more test points per task\n",
    "overlap_train = 10           # small overlap between tasks\n",
    "overlap_test  = 20          # some overlap in test sets\n",
    "obs_noise_std = 0.01        # observation noise sigma\n",
    "lmc_rank = 2                # SLFM rank let just 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "099a463c-1623-419a-8d44-70411ef5cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 = torch.tensor(np.array(hessian_matrix[idx0], dtype=np.float32), device=device)\n",
    "A1 = torch.tensor(np.array(hessian_matrix[idx1], dtype=np.float32), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "386a3ba6-c8d5-4b0e-9b77-394549475c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0527,  0.0051,  0.7544,  0.1052,  0.3904],\n",
       "        [ 0.0051,  1.2829,  0.3123, -0.2400,  0.1624],\n",
       "        [ 0.7544,  0.3123,  2.3509,  0.1673,  0.0201],\n",
       "        [ 0.1052, -0.2400,  0.1673,  3.4453,  0.2774],\n",
       "        [ 0.3904,  0.1624,  0.0201,  0.2774,  1.4418]], device='cuda:0')"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "a3d7620a-383c-4e2d-9b9f-8be02734d9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3162,  0.4711,  0.3913,  0.0794,  0.1488],\n",
       "        [ 0.4711,  2.3593, -0.0381,  0.9046,  0.0911],\n",
       "        [ 0.3913, -0.0381,  3.2369,  0.3954,  0.3434],\n",
       "        [ 0.0794,  0.9046,  0.3954,  2.8831, -0.7183],\n",
       "        [ 0.1488,  0.0911,  0.3434, -0.7183,  1.6790]], device='cuda:0')"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "f64f59cb-7bb2-45fa-829f-202d3cd62af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = A0.shape[0]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "4a39767a-17b3-4977-84c8-f418b5d1d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the qad func\n",
    "\n",
    "def f_quad(X, A):\n",
    "    # X: (N, d) tensor\n",
    "    # A: dxd tensor\n",
    "    return torch.einsum('ni,ij,nj->n', X, A, X) ## x^T @ A @ x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "8de3b319-c95f-41e3-a704-53384ff027eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## heterotopic data\n",
    "def heterotopic_split(n_train_task, n_test_task, d, overlap_train, overlap_test, x_range, seed=1234):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    lo, hi = x_range\n",
    "\n",
    "    #share subsets\n",
    "    Xtr_shared = rng.uniform(lo, hi, size=(overlap_train, d))\n",
    "    Xte_shared = rng.uniform(lo, hi, size=(overlap_test, d))\n",
    "\n",
    "    #uniques\n",
    "    def uniq(n): return rng.uniform(lo, hi, size=(n, d))\n",
    "\n",
    "    n_u_tr = max(0, n_train_task - overlap_train)\n",
    "    n_u_te = max(0, n_test_task  - overlap_test)\n",
    "    X0_tr = np.vstack([Xtr_shared, uniq(n_u_tr)])\n",
    "    X1_tr = np.vstack([Xtr_shared, uniq(n_u_tr)])\n",
    "    X0_te = np.vstack([Xte_shared, uniq(n_u_te)])\n",
    "    X1_te = np.vstack([Xte_shared, uniq(n_u_te)])\n",
    "    rng.shuffle(X0_tr); rng.shuffle(X1_tr); rng.shuffle(X0_te); rng.shuffle(X1_te)\n",
    "    to_t = lambda Z: torch.tensor(Z, dtype=dtype, device=device)\n",
    "    return [to_t(X0_tr), to_t(X1_tr)], [to_t(X0_te), to_t(X1_te)]\n",
    "\n",
    "Xtr_list, Xte_list = heterotopic_split(\n",
    "    n_train_task, n_test_task, d, overlap_train, overlap_test, x_range, seed=2025\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "2a234e11-6ee2-4a50-810f-840b0bae13dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting label for the train and test point for each function\n",
    "with torch.no_grad():\n",
    "    y0_tr = f_quad(Xtr_list[0], A0)\n",
    "    y1_tr = f_quad(Xtr_list[1], A1)\n",
    "    y0_te = f_quad(Xte_list[0], A0)\n",
    "    y1_te = f_quad(Xte_list[1], A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "e8df632b-24a4-4338-9242-9d88ebcf7cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y0_tr_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "2429f5ba-1af9-4af0-8605-863a82b37208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise for train\n",
    "\n",
    "noise = lambda shape: obs_noise_std * torch.randn(shape, dtype=dtype, device=device)\n",
    "\n",
    "y0_tr_n = y0_tr + noise(y0_tr.shape)\n",
    "y1_tr_n = y1_tr + noise(y1_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "a0f9cae2-cbe5-4237-930b-11b190c17c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack data to construct data for trainin + labeling for each data that belong to which tasks\n",
    "\n",
    "def to_long_format(x_list, y_list):\n",
    "    X = torch.cat(x_list, dim = 0)\n",
    "    y = torch.cat(y_list, dim = 0)\n",
    "    tids = []\n",
    "    \n",
    "    for t, Xt in enumerate(x_list):\n",
    "        tids.append(torch.full((Xt.size(0),), t, dtype=torch.long, device=device))\n",
    "\n",
    "    return X, torch.cat(tids, dim=0), y\n",
    "\n",
    "Xtr_long, tids_tr, ytr_long = to_long_format([Xtr_list[0], Xtr_list[1]], [y0_tr_n, y1_tr_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "2391b926-6292-476e-80cb-6a7e88f58bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the SLFM model with lowrank the rank is 1\n",
    "from gpytorch.constraints import Interval\n",
    "\n",
    "class ExactLMC_MOGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, Xtrain, tids_train, ytrain, num_tasks=2, Q=2, ranks=None):\n",
    "        lik = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        super().__init__((Xtrain, tids_train), ytrain, lik)\n",
    "        \n",
    "        self.likelihood = lik\n",
    "        self.mean = gpytorch.means.ZeroMean()\n",
    "\n",
    "        d = Xtrain.size(-1)\n",
    "        self.Q = Q ## number of latent function\n",
    "        \n",
    "        if ranks is None:\n",
    "            ranks = [1]*Q  # SLFM rank-1 if the rank is not provided \n",
    "\n",
    "        # input kernels k_q(x,x')\n",
    "        self.x_kernels = torch.nn.ModuleList([\n",
    "            gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(ard_num_dims=d)\n",
    "                # gpytorch.kernels.RBFKernel() #not ARD\n",
    "                \n",
    "            ) for _ in range(Q)\n",
    "        ])\n",
    "\n",
    "        # task cov = Aq @ Aq^T\n",
    "        self.task_kernels = torch.nn.ModuleList([\n",
    "            gpytorch.kernels.IndexKernel(num_tasks=num_tasks, rank=ranks[q], var_constraint=None) ## kernel on index task\n",
    "            for q in range(Q)\n",
    "        ])\n",
    "\n",
    "        # bounding the hyperparams\n",
    "        for q in range(Q):\n",
    "            self.x_kernels[q].base_kernel.register_constraint(\"raw_lengthscale\", Interval(0.01, 2.0))\n",
    "            self.x_kernels[q].register_constraint(\"raw_outputscale\", Interval(1e-4, 10.0))\n",
    "            self.task_kernels[q].register_constraint(\"raw_var\", Interval(1e-6, 5.0))\n",
    "\n",
    "    def forward(self, X, task_ids):\n",
    "        mean = self.mean(X)\n",
    "        K = None\n",
    "        for q in range(self.Q):\n",
    "            Kq = self.x_kernels[q](X).mul(self.task_kernels[q](task_ids)) ## hadamaard product between the covariance and extended\n",
    "            ## the output correlation matrix B\n",
    "            ## K = Sum of q [K11 K12]\n",
    "            ##              [K21 K22]   (N1+N2) x (N1 + N2)\n",
    "            K = Kq if K is None else K + Kq\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, K)\n",
    "\n",
    "Q = 2 ## the special case is they do not have anything to share -> the correlation matrix will give more weight to one latent \n",
    "## function than others\n",
    "\n",
    "ranks = [1, 1] \n",
    "\n",
    "# define the model\n",
    "model = ExactLMC_MOGP(Xtr_long, tids_tr, ytr_long, num_tasks=2, Q=Q, ranks=ranks).to(device)\n",
    "\n",
    "model.train(); model.likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "6146252e-0eaf-4e70-9da0-291441bd8d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Joint LMC] iter 100  nll=0.5699  noise=0.3030\n",
      "[Joint LMC] iter 200  nll=0.1486  noise=0.1151\n",
      "[Joint LMC] iter 300  nll=-0.2483  noise=0.0433\n",
      "[Joint LMC] iter 400  nll=-0.6074  noise=0.0164\n",
      "[Joint LMC] iter 500  nll=-0.9269  noise=0.0066\n",
      "[Joint LMC] iter 600  nll=-1.1802  noise=0.0029\n",
      "[Joint LMC] iter 700  nll=-1.3917  noise=0.0014\n",
      "[Joint LMC] iter 800  nll=-1.5809  noise=0.0007\n",
      "[Joint LMC] iter 900  nll=-1.7054  noise=0.0004\n",
      "[Joint LMC] iter 1000  nll=-1.7806  noise=0.0003\n",
      "[Joint LMC] iter 1100  nll=-1.8264  noise=0.0002\n",
      "[Joint LMC] iter 1200  nll=-1.8515  noise=0.0002\n",
      "[Joint LMC] iter 1300  nll=-1.8661  noise=0.0002\n",
      "[Joint LMC] iter 1400  nll=-1.8813  noise=0.0002\n",
      "[Joint LMC] iter 1500  nll=-1.8946  noise=0.0001\n",
      "[Joint LMC] iter 1600  nll=-1.8979  noise=0.0001\n",
      "[Joint LMC] iter 1700  nll=-1.9000  noise=0.0001\n",
      "[Joint LMC] iter 1800  nll=-1.9006  noise=0.0001\n",
      "[Joint LMC] iter 1900  nll=-1.9067  noise=0.0001\n",
      "[Joint LMC] iter 2000  nll=-1.9109  noise=0.0001\n"
     ]
    }
   ],
   "source": [
    "for it in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    with gpytorch.settings.cholesky_jitter(1e-4):\n",
    "        out = model(Xtr_long, tids_tr)\n",
    "        loss = -mll(out, ytr_long)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (it+1) % 100 == 0:\n",
    "        print(f\"[Joint LMC] iter {it+1:3d}  nll={loss.item():.4f}  noise={model.likelihood.noise.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "0e84e5be-44c0-4902-8223-90c3112cbb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B_0 (task coregionalization for component 0):\n",
      "[[13.433264 11.110017]\n",
      " [11.110017  9.395976]]\n",
      "\n",
      "B_1 (task coregionalization for component 1):\n",
      "[[0.04765284 0.27164856]\n",
      " [0.27164856 2.4076877 ]]\n"
     ]
    }
   ],
   "source": [
    "def eval_task(model, X_star, task_id, y_true):\n",
    "    model.eval(); model.likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.settings.cholesky_jitter(1e-4):\n",
    "        tids = torch.full((X_star.size(0),), task_id, dtype=torch.long, device=device)\n",
    "        pred = model.likelihood(model(X_star, tids))\n",
    "        mean = pred.mean\n",
    "        rmse = torch.sqrt(torch.mean((mean - y_true)**2)).item()\n",
    "        nll  = (-pred.log_prob(y_true)).item()\n",
    "        return rmse, nll\n",
    "\n",
    "rmse_joint_task0, nll_joint_task0 = eval_task(model, Xte_list[0], 0, y0_te)\n",
    "rmse_joint_task1, nll_joint_task1 = eval_task(model, Xte_list[1], 1, y1_te)\n",
    "\n",
    "# Q-component LMC\n",
    "\n",
    "with torch.no_grad():\n",
    "    tasks_vec = torch.tensor([0, 1], dtype=torch.long, device=device)\n",
    "\n",
    "    # each B_q (2x2) from the IndexKernel of component q\n",
    "    Bq_list = []\n",
    "    for q, tk in enumerate(model.task_kernels):\n",
    "        Bq = tk(tasks_vec).to_dense()   # torch (2,2)\n",
    "        Bq_list.append(Bq)\n",
    "        print(f\"\\nB_{q} (task coregionalization for component {q}):\")\n",
    "        print(Bq.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "498b6844-35aa-49d4-8b70-e999926c5d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of task 1: 0.009659592062234879\n",
      "RMSE of task 2: 0.011949704959988594\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE of task 1:\", rmse_joint_task0)\n",
    "print(\"RMSE of task 2:\", rmse_joint_task1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "14a7f929-cbac-42cc-8237-14a57193d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "ab6354f6-9408-4e95-ae2b-ba4b8b0a7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "\n",
    "# If you're in float32 now, double helps stability\n",
    "X0tr, X1tr = Xtr_list[0].double(), Xtr_list[1].double()\n",
    "X0te, X1te = Xte_list[0].double(), Xte_list[1].double()\n",
    "y0tr = y0_tr.double(); y1tr = y1_tr.double()\n",
    "y0te = y0_te.double(); y1te = y1_te.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "bb224d06-7fed-46fc-bda0-ec322a910ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    y0tr_n = y0_tr_n.double()\n",
    "    y1tr_n = y1_tr_n.double()\n",
    "except NameError:\n",
    "    sigma = float(obs_noise_std)\n",
    "    y0tr_n = y0tr + sigma * torch.randn_like(y0tr)\n",
    "    y1tr_n = y1tr + sigma * torch.randn_like(y1tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "f6aa630e-c8e7-49f8-bd32-9c20786f4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactSingleGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, Xtrain, ytrain):\n",
    "        lik = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        super().__init__(Xtrain, ytrain, lik)\n",
    "        self.mean_module  = gpytorch.means.ZeroMean()\n",
    "\n",
    "        # === Isotropic kernel (NO ARD) ===\n",
    "        # One lengthscale for all 5 dims; outputscale on top.\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel()   # <-- no ard_num_dims\n",
    "        )\n",
    "\n",
    "        self.likelihood = lik\n",
    "\n",
    "        # sensible inits (inside bounds)\n",
    "        self.covar_module.base_kernel.lengthscale = 0.5\n",
    "        self.covar_module.outputscale = 1.0\n",
    "        self.likelihood.noise = torch.tensor(max(1e-4, float(obs_noise_std)**2), dtype=torch.float64)\n",
    "\n",
    "        # bounds (no ARD params here)\n",
    "        from gpytorch.constraints import Interval\n",
    "        self.covar_module.base_kernel.register_constraint(\"raw_lengthscale\", Interval(0.01, 2.0))\n",
    "        self.covar_module.register_constraint(\"raw_outputscale\", Interval(1e-5, 20.0))\n",
    "        # self.likelihood.register_constraint(\"raw_noise\", Interval(1e-6, 0.5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean_x = self.mean_module(X)\n",
    "        covar_x = self.covar_module(X)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "3da60e6b-38e4-43b1-9b42-d8518a2220a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sogp(Xtr, ytr, iters=2000, lr=0.01, jitter=1e-3):\n",
    "    m = ExactSingleGP(Xtr, ytr).to(Xtr.device).double()\n",
    "    m.train(); m.likelihood.train()\n",
    "    opt = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(m.likelihood, m)\n",
    "    for it in range(iters):\n",
    "        opt.zero_grad()\n",
    "        with gpytorch.settings.cholesky_jitter(jitter):\n",
    "            out = m(Xtr)\n",
    "            loss = -mll(out, ytr)\n",
    "        if not torch.isfinite(loss): \n",
    "            print(f\"[SOGP] non-finite loss at iter {it}\"); break\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(m.parameters(), 10.0)\n",
    "        opt.step()\n",
    "        if (it+1) % 100 == 0:\n",
    "            print(f\"[SOGP] iter {it+1:3d}  nll={loss.item():.4f}  noise={m.likelihood.noise.item():.4g}\")\n",
    "    m.eval(); m.likelihood.eval()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "51d1208e-4ab0-491c-a7a2-0641f87cb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sogp(m, Xte, yte, jitter=1e-6):\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.settings.cholesky_jitter(jitter):\n",
    "        pred = m.likelihood(m(Xte))\n",
    "        mean = pred.mean\n",
    "        rmse = torch.sqrt(torch.mean((mean - yte)**2)).item()\n",
    "        nll  = (-pred.log_prob(yte)).item()\n",
    "    return rmse, nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "2498eb96-767a-4dd8-8842-1ff9106ac6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOGP] iter 100  nll=-1.5576  noise=0.0001\n",
      "[SOGP] iter 200  nll=-1.6123  noise=0.0001\n",
      "[SOGP] iter 300  nll=-1.6330  noise=0.0001\n",
      "[SOGP] iter 400  nll=-1.6476  noise=0.0001\n",
      "[SOGP] iter 500  nll=-1.6578  noise=0.0001\n",
      "[SOGP] iter 600  nll=-1.6649  noise=0.0001\n",
      "[SOGP] iter 700  nll=-1.6700  noise=0.0001\n",
      "[SOGP] iter 800  nll=-1.6736  noise=0.0001\n",
      "[SOGP] iter 900  nll=-1.6761  noise=0.0001\n",
      "[SOGP] iter 1000  nll=-1.6780  noise=0.0001\n",
      "[SOGP] iter 1100  nll=-1.6794  noise=0.0001\n",
      "[SOGP] iter 1200  nll=-1.6804  noise=0.0001\n",
      "[SOGP] iter 1300  nll=-1.6812  noise=0.0001\n",
      "[SOGP] iter 1400  nll=-1.6818  noise=0.0001\n",
      "[SOGP] iter 1500  nll=-1.6822  noise=0.0001\n",
      "[SOGP] iter 1600  nll=-1.6826  noise=0.0001\n",
      "[SOGP] iter 1700  nll=-1.6829  noise=0.0001\n",
      "[SOGP] iter 1800  nll=-1.6832  noise=0.0001\n",
      "[SOGP] iter 1900  nll=-1.6834  noise=0.0001\n",
      "[SOGP] iter 2000  nll=-1.6836  noise=0.0001\n",
      "[SOGP] iter 100  nll=-1.6005  noise=0.0001\n",
      "[SOGP] iter 200  nll=-1.6384  noise=0.0001\n",
      "[SOGP] iter 300  nll=-1.6513  noise=0.0001\n",
      "[SOGP] iter 400  nll=-1.6612  noise=0.0001\n",
      "[SOGP] iter 500  nll=-1.6685  noise=0.0001\n",
      "[SOGP] iter 600  nll=-1.6740  noise=0.0001\n",
      "[SOGP] iter 700  nll=-1.6780  noise=0.0001\n",
      "[SOGP] iter 800  nll=-1.6810  noise=0.0001\n",
      "[SOGP] iter 900  nll=-1.6833  noise=0.0001\n",
      "[SOGP] iter 1000  nll=-1.6849  noise=0.0001\n",
      "[SOGP] iter 1100  nll=-1.6862  noise=0.0001\n",
      "[SOGP] iter 1200  nll=-1.6871  noise=0.0001\n",
      "[SOGP] iter 1300  nll=-1.6879  noise=0.0001\n",
      "[SOGP] iter 1400  nll=-1.6885  noise=0.0001\n",
      "[SOGP] iter 1500  nll=-1.6890  noise=0.0001\n",
      "[SOGP] iter 1600  nll=-1.6893  noise=0.0001\n",
      "[SOGP] iter 1700  nll=-1.6897  noise=0.0001\n",
      "[SOGP] iter 1800  nll=-1.6899  noise=0.0001\n",
      "[SOGP] iter 1900  nll=-1.6901  noise=0.0001\n",
      "[SOGP] iter 2000  nll=-1.6903  noise=0.0001\n"
     ]
    }
   ],
   "source": [
    "m0 = train_sogp(X0tr, y0tr_n)\n",
    "m1 = train_sogp(X1tr, y1tr_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "81c283ac-f292-4d21-a80f-a92ffff22c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SOGP results]\n",
      "task0: RMSE=0.0117  NLL=-397.546\n",
      "task1: RMSE=0.0097  NLL=-394.495\n"
     ]
    }
   ],
   "source": [
    "rmse0, nll0 = eval_sogp(m0, X0te, y0te)\n",
    "rmse1, nll1 = eval_sogp(m1, X1te, y1te)\n",
    "\n",
    "print(\"\\n[SOGP results]\")\n",
    "print(f\"task0: RMSE={rmse0:.4f}  NLL={nll0:.3f}\")\n",
    "print(f\"task1: RMSE={rmse1:.4f}  NLL={nll1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef038483-47e3-4650-bf60-885eaddd70a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
